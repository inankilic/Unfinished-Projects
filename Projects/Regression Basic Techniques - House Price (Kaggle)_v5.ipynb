{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INITIATION & READING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoLars, ElasticNet, RidgeCV, LassoCV, LassoLarsCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, SCORERS\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "\n",
    "print('** HOUSE PRICES - ADVANCED REGRESSION TECHNIQUES **')\n",
    "print(\"-\"*25)\n",
    "\n",
    "import gc\n",
    "import os\n",
    "for x in os.listdir(\"input/\"):\n",
    "    print(x)\n",
    "print(\"-\"*25)\n",
    "\n",
    "print (\"Initiation is DONE !!\")\n",
    "print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SOME USEFULL FUNCTIONS\n",
    "\n",
    "## TIMER\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer():\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print( 'Runtime: ' + str(round(t1-t0,2)) + ' sn' )\n",
    "    \n",
    "    \n",
    "def null_detector(x):\n",
    "    missing = x.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(ascending=False, inplace=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15,6) )\n",
    "    sns.barplot( x=missing.index, y=missing.values)\n",
    "    plt.title( f'Total Column: {x.shape[1]} --- Missing Column: {len(missing)} ({round(len(missing)*100/x.shape[1],2)}%) ' )\n",
    "    plt.xticks(rotation=60)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate( '{:.2f}'.format(p.get_height()/x.shape[0]),  (p.get_x() , p.get_height() + 3), fontsize=11 )  \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def target_dist_visualizer(y):\n",
    "    import scipy.stats as st\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,4) )\n",
    "\n",
    "    sns.distplot(y, kde=True, fit=st.norm, ax=ax1)\n",
    "    ax1.set(title='Normal')\n",
    "\n",
    "    sns.distplot(y, kde=True, fit=st.lognorm, ax=ax2)\n",
    "    ax2.set(title='Log Normal')\n",
    "\n",
    "    sns.distplot(y, kde=True, fit=st.johnsonsu, ax=ax3)\n",
    "    ax3.set(title='Johnson SU')\n",
    "    plt.show()\n",
    "    print('-'*120)\n",
    "    \n",
    "    df_res = pd.DataFrame({ 'normaltest-stat': [ st.normaltest(y)[0]  ],\n",
    "                            'normaltest-p'   : [ st.normaltest(y)[1]  ],\n",
    "                            'shapiro-stat'   : [ st.shapiro(y)[0]     ],\n",
    "                            'shapiro-p'      : [ st.shapiro(y)[1]     ],\n",
    "                            'anderson-stat'  : [ st.anderson(y, dist='norm')[0] ],\n",
    "                            'anderson-crit'  : [ st.anderson(y, dist='norm')[1]  ],\n",
    "                            'anderson-sgnf'  : [ st.anderson(y, dist='norm')[2]  ]   })\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "    \n",
    "def num_col_visualizer(train, test, num_cols, target, box_limit=10):\n",
    "    i=0\n",
    "    for col in num_cols:\n",
    "        i += 1\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5) )\n",
    "        # plt.suptitle(col, fontsize=20)\n",
    "\n",
    "        ## FIRST SUBPLOT\n",
    "        if train[col].nunique() <= box_limit:\n",
    "            sns.boxplot(data=train,  x=col, y=target, ax=ax1)\n",
    "        else:\n",
    "            sns.scatterplot( data=train,  x=col, y=target, ax=ax1)\n",
    "\n",
    "        ax1.set( title = str(i) + '. ' + col + ' vs. ' + target ) \n",
    "\n",
    "        ## SECOND SUBPLOT\n",
    "        try:\n",
    "            sns.distplot(train[col], kde=True, ax=ax2)\n",
    "        except RuntimeError as re:\n",
    "            if str(re).startswith(\"Selected KDE bandwidth is 0. Cannot estiamte density.\"):\n",
    "                sns.distplot(train[col], kde=True, kde_kws={'bw': 0.1}, ax=ax2)\n",
    "            else:\n",
    "                raise re\n",
    "        ax2.set( title = 'Train --- Null: ' + str(train[col].isnull().sum()) + ' --- Unique: ' + str( train[col].nunique()  ) )\n",
    "\n",
    "        ## THIRD SUBPLOT\n",
    "        try:\n",
    "            sns.distplot(test[col], kde=True, ax=ax3)\n",
    "        except RuntimeError as re:\n",
    "            if str(re).startswith(\"Selected KDE bandwidth is 0. Cannot estiamte density.\"):\n",
    "                sns.distplot(test[col], kde=True, kde_kws={'bw': 0.1}, ax=ax3)\n",
    "            else:\n",
    "                raise re\n",
    "        ax3.set( title = 'Test --- Null: ' + str(test[col].isnull().sum())  + ' --- Unique: ' + str( test[col].nunique() ) )\n",
    "\n",
    "        plt.show()\n",
    "        print(train[col].describe(include='all'))\n",
    "        print('-'*120)    \n",
    "\n",
    "\n",
    "def str_col_visualizer(train, test, str_cols, target, box_limit=20):\n",
    "    i = 0\n",
    "    for col in str_cols:\n",
    "        i+=1\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5) )\n",
    "        # plt.suptitle(col, fontsize=20)\n",
    "\n",
    "        ## FIRST SUBPLOT\n",
    "        sns.boxplot(x=train[col].fillna('NaN'), y=train[target], ax=ax1)\n",
    "        ax1.set( title = str(i) + '. ' + col + ' vs. ' + target ) \n",
    "\n",
    "        ## SECOND SUBPLOT\n",
    "        h = train[col].value_counts()\n",
    "        sns.barplot(x=h.index, y=h.values, ax=ax2)\n",
    "        ax2.set( title = 'Train --- Null: ' + str(train[col].isnull().sum()) + ' --- Unique: ' + str( train[col].nunique()  ) )\n",
    "        \n",
    "        ## THIRD SUBPLOT\n",
    "        g = test[col].value_counts()\n",
    "        sns.barplot(x=g.index, y=g.values, ax=ax3)\n",
    "        ax3.set( title = 'Test --- Null: ' + str(test[col].isnull().sum())  + ' --- Unique: ' + str( test[col].nunique() ) )\n",
    "        \n",
    "        plt.show()\n",
    "        print(train[col].describe(include='all'))\n",
    "        print('-'*120)  \n",
    "\n",
    "def reg_with_one_exploratory_visualizer(df_train, exp_var, ind_var):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5) )\n",
    "    # plt.suptitle(col, fontsize=20)\n",
    "\n",
    "    ## FIRST SUBPLOT\n",
    "    sns.regplot(x=df_train[exp_var], y=df_train[ind_var], ax=ax1)\n",
    "\n",
    "    ## SECOND SUBPLOT\n",
    "    sns.residplot(x=df_train[exp_var], y=df_train[ind_var], ax=ax2)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def reg_result_visualizer(pred, y):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,6) )\n",
    "    \n",
    "    sns.scatterplot(pred, pred-y, ax=ax1)\n",
    "    ax1.set(title='Constant Variance')\n",
    "\n",
    "    sns.distplot(pred-y, kde=True, fit=st.norm, ax=ax2)\n",
    "    ax2.set(title='Normal Distribution')\n",
    "    \n",
    "    sns.scatterplot(range(len(y)), pred-y, ax=ax3)\n",
    "    ax3.set(title='No Autocorrelation')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'input/'\n",
    "out_path = 'output/'\n",
    "\n",
    "with timer():\n",
    "    train      = pd.read_csv( f'{path}train.csv')\n",
    "    test       = pd.read_csv( f'{path}test.csv')\n",
    "    submission = pd.read_csv( f'{path}sample_submission.csv')\n",
    "\n",
    "print('Reading Data is DONE !!')\n",
    "print(\"-\"*25)\n",
    "\n",
    "print( f'train      shape :  {train.shape}       ')\n",
    "print( f'test       shape :  {test.shape}        ')\n",
    "print( f'submission shape :  {submission.shape}  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target   = 'SalePrice'\n",
    "str_cols = [f for f in train.columns if train.dtypes[f] == 'object']\n",
    "num_cols = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "num_cols.remove(target)\n",
    "num_cols.remove('Id')\n",
    "\n",
    "print(len(str_cols))\n",
    "print(len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREATMENT OF TARGET VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part will be improved with advanced methods\n",
    "y = train[target]\n",
    "target_dist_visualizer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part will be improved with advanced methods\n",
    "y = np.log1p(train[target])\n",
    "target_dist_visualizer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p < 0.05 indicates the rejection of null hypotesis, \n",
    "## (H0: that is the series comes from a normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[target] = np.log1p(train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMERICAL COLUMN VISUALISATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train = train[ ~( train['GrLivArea']   >= 4000 ) ]\n",
    "print(train.shape)\n",
    "train = train[ ~( train['LotFrontage'] >= 200  ) ]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0, ignore_index=True )\n",
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_col_visualizer(train, test, num_cols, target, box_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical Features Simple Selection and Handling\n",
    "# Seems like ordinal but it is not\n",
    "df['fea_MSSubClass']         = df['MSSubClass'].apply(lambda x:  3 if ( (x==60) | (x==120) ) else 1 if ( (x==30) | (x==45) | (x==180) ) else 2 )\n",
    "df['fea_LotFrontage']        = df['LotFrontage'].fillna( df['LotFrontage'].mean() )\n",
    "df['fea_LotFrontage_log']    = df['fea_LotFrontage'].apply(np.log1p)\n",
    "df['fea_LotArea']            = df['LotArea']\n",
    "df['fea_LotArea_log']        = df['LotArea'].apply(np.log1p)\n",
    "df['fea_OverallQual']        = df['OverallQual']\n",
    "df['fea_OverallCond']        = df['OverallCond']\n",
    "df['fea_YearBuilt']          = df['YearBuilt']\n",
    "df['fea_YearRemodAdd']       = df['YearRemodAdd']\n",
    "df['fea_YrSold']             = df['YrSold']\n",
    "\n",
    "df['fea_MasVnrArea']         = df['MasVnrArea'].fillna(0).apply(np.log1p)\n",
    "df['fea_BsmtFinSF1']         = df['BsmtFinSF1'].fillna(0)\n",
    "df['fea_BsmtFinSF2']         = df['BsmtFinSF2'].fillna(0)\n",
    "df['fea_BsmtUnfSF']          = df['BsmtUnfSF'].fillna(0)\n",
    "df['fea_BsmtUnfSF_log']      = df['BsmtUnfSF'].fillna(0).apply(np.log1p)\n",
    "df['fea_TotalBsmtSF']        = df['TotalBsmtSF'].fillna( df['TotalBsmtSF'].mean() )\n",
    "df['fea_TotalBsmtSF_log']    = df['TotalBsmtSF'].fillna( df['TotalBsmtSF'].mean() ).apply(np.log1p)\n",
    "\n",
    "\n",
    "df['fea_1stFlrSF']           = df['1stFlrSF']\n",
    "df['fea_1stFlrSF_log']       = df['1stFlrSF'].apply(np.log1p)\n",
    "df['fea_2ndFlrSF']           = df['2ndFlrSF']\n",
    "df['fea_2ndFlrSF_log']       = df['2ndFlrSF'].apply(np.log1p)\n",
    "# df['fea_LowQualFinSF']       = df['LowQualFinSF']\n",
    "df['fea_GrLivArea']          = df['GrLivArea']\n",
    "df['fea_GrLivArea_log']      = df['GrLivArea'].apply(np.log1p)\n",
    "\n",
    "df['fea_BsmtFullBath']       = df['BsmtFullBath'].fillna(0)\n",
    "df['fea_BsmtHalfBath']       = df['BsmtHalfBath'].fillna(0)\n",
    "df['fea_BsmtTotalBath']      = df['BsmtHalfBath'].fillna(0) + (df['BsmtHalfBath'].fillna(0) / 2)\n",
    "\n",
    "df['fea_FullBath']           = df['FullBath']\n",
    "df['fea_HalfBath']           = df['HalfBath']\n",
    "df['fea_TotalBath']          = df['FullBath'].fillna(0) + (df['HalfBath'].fillna(0) / 2)\n",
    "\n",
    "df['fea_BedroomAbvGr']       = df['BedroomAbvGr']\n",
    "df['fea_KitchenAbvGr']       = df['KitchenAbvGr'] \n",
    "df['fea_TotRmsAbvGrd']       = df['TotRmsAbvGrd']\n",
    "\n",
    "df['fea_Fireplaces']         = df['Fireplaces']\n",
    "df['fea_GarageYrBlt']        = df['GarageYrBlt'].fillna( df['GarageYrBlt'].mean() )\n",
    "df['fea_GarageCars']         = df['GarageCars'].fillna(0)\n",
    "\n",
    "df['fea_GarageArea']         = df['GarageArea'].fillna(0)\n",
    "df['fea_GarageArea_log']     = df['GarageArea'].apply(np.log1p)\n",
    "df['fea_WoodDeckSF']         = df['WoodDeckSF']\n",
    "\n",
    "df['fea_OpenPorchSF']        = df['OpenPorchSF']\n",
    "df['fea_EnclosedPorch']      = df['EnclosedPorch']\n",
    "df['fea_3SsnPorch']          = df['3SsnPorch']\n",
    "df['fea_ScreenPorch']        = df['ScreenPorch']\n",
    "df['fea_TotalPorch']         = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n",
    "\n",
    "df['fea_PoolArea']           = df['PoolArea']\n",
    "df['fea_MiscVal']            = df['MiscVal']\n",
    "\n",
    "df['fea_MoSold']             = df['MoSold']\n",
    "df['fea_YrSold']             = df['YrSold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_col_visualizer(train, test, str_cols, target, box_limit=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some categorical features as ordered numbers when there is information in the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## String Features (Some nominal some ordinal, treat carefully !! )\n",
    "df['fea_MSZoning']         = df['MSZoning'].fillna( df['MSZoning'].value_counts().index[0] )\n",
    "df['fea_MSZoning']         = df['fea_MSZoning'].map( dict(df.groupby(['fea_MSZoning'])['SalePrice'].median().sort_values()) )\n",
    "\n",
    "tmp = pd.get_dummies(df['Street'], prefix='fea_Street')\n",
    "tmp.drop('fea_Street_Grvl', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "tmp = pd.get_dummies(df['Alley'], prefix='fea_Alley')\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_LotShape']   =  df['LotShape'].map(  {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4} )\n",
    "\n",
    "tmp = pd.get_dummies(df['LandContour'], prefix='fea_LandContour')\n",
    "tmp.drop('fea_LandContour_Lvl', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "## Skip Utilities\n",
    "\n",
    "# No Hope\n",
    "tmp = pd.get_dummies(df['LotConfig'], prefix='fea_LotConfig')\n",
    "tmp.drop('fea_LotConfig_Inside', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "# No Hope\n",
    "df['fea_LandSlope']   =  df['LandSlope'].map( {\"Sev\" : 3, \"Mod\" : 2, \"Gtl\" : 1} )\n",
    "\n",
    "df['fea_Neighborhood']  = df['Neighborhood'].apply(lambda x: 3 if ( (x=='NoRidge') | (x=='NridgHt') | (x=='StoneBr') ) else\n",
    "                                                            1 if ( (x=='IDOTRR')  | (x=='MeadowV') | (x=='BrDale' ) ) else 2 )\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['Condition1'], prefix='fea_Condition1')\n",
    "tmp.drop('fea_Condition1_Norm', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['Condition2'], prefix='fea_Condition2')\n",
    "tmp.drop('fea_Condition2_Norm', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['BldgType'], prefix='fea_BldgType')\n",
    "tmp.drop('fea_BldgType_1Fam', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_HouseStyle']         = df['HouseStyle'].map( dict(df.groupby(['HouseStyle'])['SalePrice'].median().sort_values()) )\n",
    "\n",
    "# Skip RoofStyle\n",
    "# Skip RoofMatl\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['Exterior1st'], prefix='fea_Exterior1st')\n",
    "tmp.drop( ['fea_Exterior1st_ImStucc', 'fea_Exterior1st_Stone' ], axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['Exterior2nd'], prefix='fea_Exterior2nd')\n",
    "tmp.drop( ['fea_Exterior2nd_Other'], axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "tmp = pd.get_dummies(df['MasVnrType'], prefix='fea_MasVnrType')\n",
    "tmp.drop('fea_MasVnrType_None', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "\n",
    "df['fea_ExterQual']   =  df['ExterQual'].map(  {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5} )\n",
    "df['fea_ExterCond']   =  df['ExterCond'].map(  {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5} )\n",
    "\n",
    "tmp = pd.get_dummies(df['Foundation'], prefix='fea_Foundation')\n",
    "tmp.drop('fea_Foundation_Stone', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_BsmtQual']   =  df['BsmtQual'].map(  {np.nan:-1, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5} )\n",
    "\n",
    "df['fea_BsmtCond']   =  df['BsmtCond'].map( {np.nan:-1, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}  )\n",
    "\n",
    "df['fea_BsmtExposure']   =  df['BsmtExposure'].map(   {np.nan:-1, \"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3} )\n",
    "\n",
    "df['fea_BsmtFinType1']   =  df['BsmtFinType1'].map(   {np.nan:-1, \"Unf\" : 0, \"LwQ\" : 1, \"Rec\" : 2, \"BLQ\" : 3, \"ALQ\" : 4, \"GLQ\" : 5} )\n",
    "df['fea_BsmtFinType1_GLQ']   =  df['BsmtFinType1'].map(   {np.nan:-1, \"Unf\" : 0, \"LwQ\" : 0, \"Rec\" : 0, \"BLQ\" : 0, \"ALQ\" : 0, \"GLQ\" : 1} )\n",
    "\n",
    "df['fea_BsmtFinType2']   =  df['BsmtFinType2'].map(   {np.nan:-1, \"Unf\" : 0, \"LwQ\" : 1, \"Rec\" : 2, \"BLQ\" : 3, \"ALQ\" : 4, \"GLQ\" : 5} )\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['Heating'], prefix='fea_Heating')\n",
    "tmp.drop(['fea_Heating_OthW', 'fea_Heating_Floor'], axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_HeatingQC']   =  df['HeatingQC'].map(   {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5} )\n",
    "\n",
    "tmp = pd.get_dummies(df['CentralAir'], prefix='fea_CentralAir')\n",
    "tmp.drop('fea_CentralAir_N', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_Electrical']   =  df['Electrical'].map(   {np.nan:5, \"Mix\" : 1, \"FuseP\" : 2, \"FuseF\" : 3, \"FuseA\" : 4, \"SBrkr\" : 5} )\n",
    "\n",
    "df['fea_KitchenQual']   =  df['KitchenQual'].map( {np.nan:0, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}  )\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['Functional'], prefix='fea_Functional')\n",
    "tmp.drop('fea_Functional_Typ', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_FireplaceQu']   =  df['FireplaceQu'].map( {np.nan:0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}  )\n",
    "\n",
    "tmp = pd.get_dummies(df['GarageType'], prefix='fea_GarageType')\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_GarageFinish']   =  df['GarageFinish'].map( {np.nan:0, \"Unf\" : 1, \"RFn\" : 2, \"Fin\": 3}  )\n",
    "\n",
    "df['fea_GarageQual']   =  df['GarageQual'].map( {np.nan:0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}  )\n",
    "\n",
    "df['fea_GarageCond']   =  df['GarageCond'].map( {np.nan:0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}  )\n",
    "\n",
    "df['fea_PavedDrive']   =  df['PavedDrive'].map( { \"N\" : 1, \"P\" : 2, \"Y\": 3}  )\n",
    "\n",
    "df['fea_PoolQC']   =  df['PoolQC'].map( {np.nan:0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}  )\n",
    "\n",
    "#Skip Fence\n",
    "\n",
    "#No Hope\n",
    "tmp = pd.get_dummies(df['MiscFeature'], prefix='fea_MiscFeature')\n",
    "tmp.drop(['fea_MiscFeature_Shed', 'fea_MiscFeature_TenC'], axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "tmp = pd.get_dummies(df['SaleType'], prefix='fea_SaleType')\n",
    "tmp.drop('fea_SaleType_WD', axis=1, inplace=True)\n",
    "df = pd.concat( [df, tmp], axis=1 )\n",
    "\n",
    "df['fea_SaleCondition']         = df['SaleCondition'].map( dict(df.groupby(['SaleCondition'])['SalePrice'].median().sort_values()) )\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMBINATIONS OF EXISTING FEATURES\n",
    "\n",
    "### OVERALL QUALITIES \n",
    "# Overall quality of the house\n",
    "df[\"fea_OverallGrade\"] = df[\"fea_OverallQual\"] * df[\"fea_OverallCond\"]\n",
    "# Overall quality of the garage\n",
    "df[\"fea_GarageGrade\"] = df[\"fea_GarageQual\"] * df[\"fea_GarageCond\"]\n",
    "# Overall quality of the exterior\n",
    "df[\"fea_ExterGrade\"] = df[\"fea_ExterQual\"] * df[\"fea_ExterCond\"]\n",
    "\n",
    "### OVERALL SCORES\n",
    "# Overall kitchen score\n",
    "df[\"fea_KitchenScore\"] = df[\"fea_KitchenAbvGr\"] * df[\"fea_KitchenQual\"]\n",
    "# Overall fireplace score\n",
    "df[\"fea_FireplaceScore\"] = df[\"fea_Fireplaces\"] * df[\"fea_FireplaceQu\"]\n",
    "# Overall garage score\n",
    "df[\"fea_GarageScore\"] = df[\"fea_GarageArea\"] * df[\"fea_GarageQual\"]\n",
    "# Overall pool score\n",
    "df[\"fea_PoolScore\"] = df[\"fea_PoolArea\"] * df[\"fea_PoolQC\"]\n",
    "\n",
    "### TOTAL NUMBERS\n",
    "\n",
    "# Total SF for house (incl. basement)\n",
    "df[\"fea_AllSF\"] = df[\"fea_GrLivArea\"] + df[\"fea_TotalBsmtSF\"]\n",
    "df[\"fea_AllSF_log\"] = df[\"fea_AllSF\"].apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_cols = [col for col in df.columns if col.startswith('fea_')]\n",
    "print(len(fea_cols))\n",
    "df = df[fea_cols + ['SalePrice']]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df.copy()\n",
    "df_scaled[fea_cols] = MinMaxScaler().fit_transform(df_scaled[fea_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = df_scaled[df_scaled['SalePrice'].notnull()]\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = df_train.corr()['SalePrice'].reset_index().sort_values('SalePrice', ascending=False)\n",
    "pd.concat([spearman.head(), spearman.tail()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BASIC REGRESSION WITH ONE EXPLORATORY VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = ['fea_AllSF']\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_with_one_exploratory_visualizer(df_train, exp_vars, target)\n",
    "print(\"-\"*25)\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the residual by predicted plot, we see that the residuals are randomly scattered around the center line of zero, with no obvious non-random pattern.\n",
    "# Residuals have constant variance\n",
    "\n",
    "# Residuals are approximately normally distributed\n",
    "\n",
    "# No autocorrelation!! Residuals are independent one another, if doubt apply Durbin Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REGRESSION WITH ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = fea_cols.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF dataframe \n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_train[fea_cols].columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_train[fea_cols].values, i) \n",
    "                          for i in range(len(df_train[fea_cols].columns))]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. REGRESSION WITH FEATURES - BACKWARD ELIMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[fea_cols]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE)\n",
    "print(len(selected_features_BE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = selected_features_BE.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF dataframe \n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_train[selected_features_BE].columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_train[selected_features_BE].values, i) for i in range(len(df_train[selected_features_BE].columns))]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,16) )\n",
    "sns.heatmap( df_train[selected_features_BE].corr(),  annot=True  )\n",
    "plt.title('xx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. REGRESSION WITH FEATURES DETERMINED BY CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = df_train.corr()['SalePrice'].reset_index().sort_values('SalePrice', ascending=False)\n",
    "corred  = spearman[ (spearman['SalePrice'] >= 0.5) | (spearman['SalePrice'] <= -0.5) ]\n",
    "\n",
    "selected_features_corr = corred['index'].to_list()\n",
    "selected_features_corr.remove(target)\n",
    "print(len(selected_features_corr))\n",
    "print(selected_features_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10) )\n",
    "sns.heatmap( df_train[selected_features_corr].corr(),  annot=True   )\n",
    "plt.title('Normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['fea_AllSF_log', 'fea_GrLivArea', 'fea_GrLivArea_log', 'fea_1stFlrSF', 'fea_FullBath',\n",
    "                  'fea_GarageArea', 'fea_FireplaceQu', 'fea_ExterQual', 'fea_GarageYrBlt', 'fea_GarageCars', 'fea_TotalBsmtSF']\n",
    "for cols in cols_to_remove:\n",
    "    selected_features_corr.remove(cols)\n",
    "print(len(selected_features_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10) )\n",
    "sns.heatmap( df_train[selected_features_corr].corr(),  annot=True   )\n",
    "plt.title('Normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = selected_features_corr.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[selected_features_corr]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features_corr_BE = cols\n",
    "print(selected_features_corr_BE)\n",
    "print(len(selected_features_corr_BE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = selected_features_corr_BE.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_train[features_with_correlation].columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_train[features_with_correlation].values, i) \n",
    "                          for i in range(len(df_train[features_with_correlation].columns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. REGRESSION WITH FEATURES DETERMINED BY f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[fea_cols]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "fs = SelectKBest(score_func=f_regression, k='all')\n",
    "fs.fit(X, y)\n",
    "X_fs = fs.transform(X)\n",
    "\n",
    "df_fs = pd.DataFrame()\n",
    "df_fs['Feature'] = X.columns\n",
    "df_fs['fs_score'] = fs.scores_\n",
    "df_fs['fs_pvalue'] = fs.pvalues_\n",
    "\n",
    "df_fs.sort_values('fs_score', ascending=False).head(30).plot( kind='bar', x='Feature', y='fs_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "features_from_fs_score = df_fs.sort_values('fs_score', ascending=False).head(k)['Feature'].to_list()\n",
    "# print(df_fs.sort_values('fs_score', ascending=False).head(k)['Feature'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = features_from_fs_score.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[features_from_fs_score]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "features_from_fs_score_BE = cols\n",
    "print(features_from_fs_score_BE)\n",
    "print(len(features_from_fs_score_BE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = features_from_fs_score_BE.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_train[features_from_fs_score].columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_train[features_from_fs_score].values, i) \n",
    "                          for i in range(len(df_train[features_from_fs_score].columns))]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8) )\n",
    "sns.heatmap( df_train[features_from_fs_score_BE].corr(),  annot=True   )\n",
    "plt.title('Normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. REGRESSION WITH FEATURES DETERMINED BY Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[fea_cols]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "mir = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "mir.fit(X, y)\n",
    "X_mir = mir.transform(X)\n",
    "\n",
    "df_mir = pd.DataFrame()\n",
    "df_mir['Feature'] = X.columns\n",
    "df_mir['mir_score'] = mir.scores_\n",
    "df_mir['mir_pvalue'] = mir.pvalues_\n",
    "\n",
    "df_mir.sort_values('mir_score', ascending=False).head(30).plot( kind='bar', x='Feature', y='mir_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "features_from_mir_score = df_mir.sort_values('mir_score', ascending=False).head(k)['Feature'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = features_from_mir_score.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[features_from_mir_score]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "features_from_mir_score_BE = cols\n",
    "print(features_from_mir_score_BE)\n",
    "print(len(features_from_mir_score_BE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = features_from_mir_score_BE.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "##############################\n",
    "##### SCIKIT-LEARN PART\n",
    "##############################\n",
    "\n",
    "reg          = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "##############################\n",
    "##### STATMODELS PART\n",
    "##############################\n",
    "\n",
    "X            = sm.add_constant(df_train[exp_vars])\n",
    "y            = df_train['SalePrice']\n",
    "\n",
    "model        = sm.OLS(y, X).fit() \n",
    "pred         = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "\n",
    "##############################\n",
    "##### RESIDUAL VISUALIZER\n",
    "##############################\n",
    "\n",
    "print(\"-\"*25)\n",
    "reg_result_visualizer(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8) )\n",
    "sns.heatmap( df_train[features_from_mir_score_BE].corr(),  annot=True   )\n",
    "plt.title('Normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_train[features_from_mir_score].columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_train[features_from_mir_score].values, i) \n",
    "                          for i in range(len(df_train[features_from_mir_score].columns))]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = fea_cols.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "alpha_space = np.arange(0.1, 20, 0.1) # np.logspace(-12, -1, 400)\n",
    "\n",
    "ridge = RidgeCV(alphas = alpha_space, store_cv_values = True)\n",
    "ridge.fit(X, y)\n",
    "cv_values = ridge.cv_values_.mean(axis=0)\n",
    "\n",
    "alpha = ridge.alpha_\n",
    "print(\"Best alpha :\", alpha)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(alpha_space, cv_values, label=\"ridge\")\n",
    "\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"mean squared error\")\n",
    "plt.title(\"RidgeCV Alpha Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_space = np.arange(0.1, 20, 0.1) # np.logspace(-12, -1, 400)\n",
    "scores = []\n",
    "model = Ridge()\n",
    "for alpha in alpha_space:\n",
    "    model.set_params(alpha=alpha)\n",
    "    score = cross_val_score(model, X, y, cv=5)\n",
    "    scores.append(score.mean())\n",
    "\n",
    "res = pd.DataFrame({ 'alpha': alpha_space, 'score': scores })\n",
    "print(\"Best alpha :\", float(res[res['score'] == res['score'].max()]['alpha']))   \n",
    "\n",
    "plt.plot(alpha_space, scores, label='ridge')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2')\n",
    "plt.title(\"Manual Ridge Alpha Scoring\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg          = Ridge(alpha=alpha)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred       = reg.predict(X_test)\n",
    "r2           = reg.score(X_test, y_test)\n",
    "rmse         = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "reg.fit(X, y)\n",
    "y_pred_train = reg.predict(X)\n",
    "r2_train     = reg.score(X, y)\n",
    "rmse_train   = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "\n",
    "cv_scores    = cross_val_score(reg, X, y, cv=5)\n",
    "cv_score     = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Train R^2:         {r2_train}  \")\n",
    "print(f\"Train RMSE:        {rmse_train}\")\n",
    "print(f\"Test (0.3) R^2:    {r2}        \")\n",
    "print(f\"Test (0.3) RMSE:   {rmse}      \")\n",
    "print(f\"5-Fold Avg CV:     {cv_score}  \")\n",
    "print(f\"5-Fold CV Scores:  {cv_scores} \")\n",
    "\n",
    "print('-'*25)\n",
    "print(f'{sum(reg.coef_ == 0)} features eliminated!')\n",
    "print(f'{sum(reg.coef_ != 0)} features utilized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = fea_cols.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "alpha_space = np.arange(-5, 5) # np.logspace(-12, -1, 400)\n",
    "\n",
    "lasso = LassoCV(alphas = alpha_space)\n",
    "lasso.fit(X, y)\n",
    "cv_values = lasso.mse_path_.mean(axis=1)\n",
    "\n",
    "alpha = lasso.alpha_\n",
    "print(\"Best alpha :\", alpha)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(alpha_space, cv_values, label=\"lasso\")\n",
    "\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"mean squared error\")\n",
    "plt.title(\"LassoCV Alpha Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_space = np.arange(-5, 5, 0.1) # np.logspace(-12, -1, 400)\n",
    "scores = []\n",
    "model = Lasso()\n",
    "for alpha in alpha_space:\n",
    "    model.set_params(alpha=alpha)\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    scores.append(score.mean())\n",
    "\n",
    "res = pd.DataFrame({ 'alpha': alpha_space, 'score': scores })\n",
    "print(\"Best alpha :\", float(res[res['score'] == res['score'].max()]['alpha']))   \n",
    "\n",
    "plt.plot(alpha_space, scores, label='lasso')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2')\n",
    "plt.title(\"Manual Ridge Alpha Scoring\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. LASSOLARS REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = fea_cols.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "\n",
    "lassoLars = LassoLarsCV()\n",
    "lassoLars.fit(X, y)\n",
    "cv_values = lassoLars.mse_path_.mean(axis=1)\n",
    "\n",
    "alpha = lassoLars.alpha_\n",
    "print(\"Best alpha :\", alpha)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(lassoLars.cv_alphas_, cv_values, label=\"lassoLars\")\n",
    "\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"mean squared error\")\n",
    "plt.title(\"LassoLarsCV Alpha Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. ELASTICNET REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = fea_cols.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "alpha_space = np.arange(0, 5, 0.01) # np.logspace(-12, -1, 400)\n",
    "\n",
    "ElasticNet = ElasticNetCV()\n",
    "ElasticNet.fit(X, y)\n",
    "cv_values = ElasticNet.mse_path_.mean(axis=1)\n",
    "\n",
    "alpha = ElasticNet.alpha_\n",
    "print(\"Best alpha :\", alpha)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(ElasticNet.alphas_, cv_values, label=\"ElasticNet\")\n",
    "\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"mean squared error\")\n",
    "plt.title(\"LassoLarsCV Alpha Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. DECISION TREE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "exp_vars = fea_cols.copy()\n",
    "\n",
    "X = df_train[exp_vars].values\n",
    "y = df_train['SalePrice'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=28)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "                           min_samples_leaf=0.13,\n",
    "                           random_state=28)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Test set MSE of dt: {:.4f}\".format(mse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=5, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "print(MSE_CV_scores.mean())\n",
    "MSE_CV_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)\n",
    "y_pred_train = dt.predict(X_train)\n",
    "MSE(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. BAGGING REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "                           min_samples_leaf=0.13,\n",
    "                           random_state=28)\n",
    "br = BaggingRegressor(base_estimator=dt, \n",
    "                       n_estimators=50,\n",
    "                       oob_score=True,\n",
    "                       random_state=28)\n",
    "\n",
    "\n",
    "br.fit(X_train, y_train)\n",
    "y_pred = br.predict(X_test)\n",
    "mse_br = MSE(y_test, y_pred)\n",
    "\n",
    "print(\"Test set MSE of dt: {:.4f}\".format(mse_br))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. RANDOM FOREST REGRESSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "                           random_state=2)\n",
    "        \n",
    "rf.fit(X_train, y_train)                           \n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "mse_rf = MSE(y_test, y_pred)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set MSE of rf: {:.4f}'.format(mse_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame( {'feature': fea_cols,\n",
    "                             'importance': rf.feature_importances_ } )\n",
    "    \n",
    "    \n",
    "importances_sorted = importances.sort_values('importance', ascending=False)\n",
    "importances_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a horizontal barplot of importances_sorted\n",
    "sns.barplot(x= importances_sorted['importance'].head(20),\n",
    "            y= importances_sorted['feature'].head(20), orient='h',)\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. ADABOOST REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4, random_state=28)\n",
    "ada = AdaBoostRegressor(base_estimator=dt, n_estimators=100, random_state=1)\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ada.predict(X_test)\n",
    "\n",
    "mse_ada = MSE(y_test, y_pred)\n",
    "\n",
    "print('Test set MSE of ada: {:.4f}'.format(mse_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame( {'feature': fea_cols,\n",
    "                             'importance': ada.feature_importances_ } )\n",
    "    \n",
    "    \n",
    "importances_sorted = importances.sort_values('importance', ascending=False)\n",
    "importances_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. GRADIENT BOOSTING REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "gb = GradientBoostingRegressor(max_depth=3,\n",
    "                               n_estimators=200,\n",
    "                               random_state=28)\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "print('Test set MSE of gb: {:.3f}'.format(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame( {'feature': fea_cols,\n",
    "                             'importance': ada.feature_importances_ } )\n",
    "    \n",
    "    \n",
    "importances_sorted = importances.sort_values('importance', ascending=False)\n",
    "importances_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "sgbr = GradientBoostingRegressor(max_depth=3, \n",
    "                                 subsample=0.8,\n",
    "                                 max_features=0.75,\n",
    "                                 n_estimators=200,                                \n",
    "                                 random_state=28)\n",
    "\n",
    "sgbr.fit(X_train, y_train)\n",
    "y_pred = sgbr.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "print('Test set MSE of gb: {:.3f}'.format(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame( {'feature': fea_cols,\n",
    "                             'importance': ada.feature_importances_ } )\n",
    "    \n",
    "    \n",
    "importances_sorted = importances.sort_values('importance', ascending=False)\n",
    "importances_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ................TO BE CONTINUED.............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id'] = train['item_id'] + '_' + train['store_id']\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making data usable in models\n",
    "with timer():\n",
    "    train = pd.melt( train,\n",
    "                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                     value_vars = [col for col in train.columns if col.startswith(\"d_\")],\n",
    "                     var_name = \"d\",\n",
    "                     value_name = \"sales\")\n",
    "with timer():\n",
    "    train['d']     = train['d'].str.extract('(\\d+)').astype('int16')\n",
    "    train['sales'] = train['sales'].astype('int16')\n",
    "\n",
    "\n",
    "with timer():\n",
    "    train.sort_values(['id', 'd'], inplace=True)\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the 28 days we try to forecast\n",
    "with timer():\n",
    "    tmp = train.groupby('id', as_index=False ).tail(28)\n",
    "    tmp['d'] = tmp['d'] + 28\n",
    "    tmp['sales'] = np.nan\n",
    "    print(tmp.shape)\n",
    "\n",
    "with timer():\n",
    "    train = pd.concat([train,tmp], axis=0, sort=False, ignore_index=True)\n",
    "\n",
    "del tmp; gc.collect()\n",
    "\n",
    "with timer():\n",
    "    train.sort_values(['id', 'd'], inplace=True)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding lag days\n",
    "def lagger(train, lag_day):\n",
    "    train[f'sales_lag_{lag_day}'] = train[['id', 'd', 'sales']].groupby('id')['sales'].transform( lambda x: x.shift(lag_day) ).astype('float16')\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5,6,7,8,12,13,14,15,28]:\n",
    "    train = lagger(train,i)\n",
    "    print(i, 'DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CALENDAR DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calendar.shape)\n",
    "print('-'*25)\n",
    "calendar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer():\n",
    "    calendar['d'] = calendar['d'].str.extract('(\\d+)').astype('int16')\n",
    "    \n",
    "    calendar['day'] = pd.to_datetime(calendar['date']).dt.day\n",
    "    \n",
    "    for col in ['wday', 'month', 'wm_yr_wk', 'year']:\n",
    "        calendar[col] = calendar[col].astype('int16')\n",
    "    \n",
    "    calendar.drop(['date', 'weekday'], axis=1, inplace=True)\n",
    "        \n",
    "print(calendar.shape)\n",
    "print('-'*25)\n",
    "calendar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT FORGET TO ADD SNAP DATA\n",
    "snap = calendar[['d', 'snap_CA', 'snap_TX', 'snap_WI']]\n",
    "\n",
    "snap = pd.melt( snap,\n",
    "                     id_vars = ['d'],\n",
    "                     value_vars = [col for col in snap.columns if col.startswith(\"snap_\")],\n",
    "                     var_name = \"state_id\",\n",
    "                     value_name = \"is_snap\")\n",
    "\n",
    "snap['state_id'] = snap['state_id'].str[5:].astype('category')\n",
    "snap['is_snap'] = snap['is_snap'].astype('int8')\n",
    "\n",
    "print(snap.shape)\n",
    "print('-'*25)\n",
    "snap.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.drop(['snap_CA', 'snap_TX', 'snap_WI'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.get_dummies(calendar['event_name_1'], dtype ='int8' )\n",
    "calendar = pd.concat([calendar, tmp], axis=1)\n",
    "\n",
    "tmp = pd.get_dummies(calendar['event_type_1'], dtype ='int8' )\n",
    "calendar = pd.concat([calendar, tmp], axis=1)\n",
    "\n",
    "\n",
    "tmp = pd.get_dummies(calendar['event_name_2'], prefix = '2', dtype ='int8' )\n",
    "calendar = pd.concat([calendar, tmp], axis=1)\n",
    "\n",
    "tmp = pd.get_dummies(calendar['event_type_2'], prefix = '2', dtype ='int8' )\n",
    "calendar = pd.concat([calendar, tmp], axis=1)\n",
    "\n",
    "calendar.drop(['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'], axis=1, inplace=True)\n",
    "del tmp; gc.collect()\n",
    "\n",
    "print(calendar.shape)\n",
    "print('-'*25)\n",
    "calendar.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar['Cinco De Mayo']    = calendar['Cinco De Mayo']   + calendar['2_Cinco De Mayo']\n",
    "calendar['Easter']           = calendar['Easter']          + calendar['2_Easter']\n",
    "calendar['Father\\'s day']    = calendar['Father\\'s day']   + calendar['2_Father\\'s day']\n",
    "calendar['OrthodoxEaster']   = calendar['OrthodoxEaster']  + calendar['2_OrthodoxEaster']\n",
    "calendar['Cultural']         = calendar['Cultural']        + calendar['2_Cultural']\n",
    "calendar['Religious']        = calendar['Religious']       + calendar['2_Religious']\n",
    "\n",
    "calendar.drop([col for col in calendar.columns if col.startswith('2_')], axis=1, inplace=True)\n",
    "\n",
    "print(calendar.shape)\n",
    "print('-'*25)\n",
    "calendar.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.loc[ (calendar['d'] >= 185 )  & ( calendar['d'] < 185 + 29 ), 'Ramadan starts' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 539 )  & ( calendar['d'] < 539 + 29 ), 'Ramadan starts' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 893 )  & ( calendar['d'] < 893 + 29 ), 'Ramadan starts' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1248)  & ( calendar['d'] < 1248 + 29), 'Ramadan starts' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1602)  & ( calendar['d'] < 1602 + 29), 'Ramadan starts' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1957)                                , 'Ramadan starts' ]  = 1\n",
    "\n",
    "calendar.loc[ (calendar['d'] >= 185 )  & ( calendar['d'] < 185 + 29 ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 539 )  & ( calendar['d'] < 539 + 29 ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 893 )  & ( calendar['d'] < 893 + 29 ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1248)  & ( calendar['d'] < 1248 + 29), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1602)  & ( calendar['d'] < 1602 + 29), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1957)                                , 'Religious' ]  = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.loc[ (calendar['d'] >= 215 - 1  )  & ( calendar['d'] <= 215 + 1  ), 'Eid al-Fitr' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 569 - 1  )  & ( calendar['d'] <= 569 + 1  ), 'Eid al-Fitr' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 923 - 1  )  & ( calendar['d'] <= 923 + 1  ), 'Eid al-Fitr' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1278 - 1 )  & ( calendar['d'] <= 1278 + 1 ), 'Eid al-Fitr' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1632 - 1 )  & ( calendar['d'] <= 1632 + 1 ), 'Eid al-Fitr' ]  = 1\n",
    "\n",
    "calendar.loc[ (calendar['d'] >= 215 - 1  )  & ( calendar['d'] <= 215 + 1  ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 569 - 1  )  & ( calendar['d'] <= 569 + 1  ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 923 - 1  )  & ( calendar['d'] <= 923 + 1  ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1278 - 1 )  & ( calendar['d'] <= 1278 + 1 ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1632 - 1 )  & ( calendar['d'] <= 1632 + 1 ), 'Religious' ]  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.loc[ (calendar['d'] >= 283   )  & ( calendar['d'] <= 283 + 3  ), 'EidAlAdha' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 637   )  & ( calendar['d'] <= 637 + 3  ), 'EidAlAdha' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 991   )  & ( calendar['d'] <= 991 + 3  ), 'EidAlAdha' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1345  )  & ( calendar['d'] <= 1345 + 3 ), 'EidAlAdha' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1700  )  & ( calendar['d'] <= 1700 + 3 ), 'EidAlAdha' ]  = 1\n",
    "\n",
    "calendar.loc[ (calendar['d'] >= 283   )  & ( calendar['d'] <= 283 + 3  ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 637   )  & ( calendar['d'] <= 637 + 3  ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 991   )  & ( calendar['d'] <= 991 + 3  ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1345  )  & ( calendar['d'] <= 1345 + 3 ), 'Religious' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1700  )  & ( calendar['d'] <= 1700 + 3 ), 'Religious' ]  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.loc[ (calendar['d'] >= 123  )  & ( calendar['d'] <= 135  ), 'NBAFinalsStart' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 501  )  & ( calendar['d'] <= 510  ), 'NBAFinalsStart' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 860  )  & ( calendar['d'] <= 874  ), 'NBAFinalsStart' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1224 )  & ( calendar['d'] <= 1234 ), 'NBAFinalsStart' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1588 )  & ( calendar['d'] <= 1600 ), 'NBAFinalsStart' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1952 )  & ( calendar['d'] <= 1969 ), 'NBAFinalsStart' ]  = 1\n",
    "\n",
    "calendar.loc[ (calendar['d'] >= 123  )  & ( calendar['d'] <= 135  ), 'Sporting' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 501  )  & ( calendar['d'] <= 510  ), 'Sporting' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 860  )  & ( calendar['d'] <= 874  ), 'Sporting' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1224 )  & ( calendar['d'] <= 1234 ), 'Sporting' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1588 )  & ( calendar['d'] <= 1600 ), 'Sporting' ]  = 1\n",
    "calendar.loc[ (calendar['d'] >= 1952 )  & ( calendar['d'] <= 1969 ), 'Sporting' ]  = 1\n",
    "\n",
    "calendar.drop(['NBAFinalsEnd'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging calendar \n",
    "with timer():\n",
    "    train = train.merge(calendar, on='d', how='left')\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calendar; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. PRICE DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prices.shape)\n",
    "print('-'*25)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices['sell_price_over_mean']   = (prices['sell_price'] / prices.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')   ) - 1\n",
    "prices['sell_price_over_median'] = (prices['sell_price'] / prices.groupby(['store_id', 'item_id'])['sell_price'].transform('median') ) - 1\n",
    "\n",
    "prices['sell_price_over_mean']   = prices['sell_price_over_mean'  ].astype('float16')\n",
    "prices['sell_price_over_median'] = prices['sell_price_over_median'].astype('float16')\n",
    "\n",
    "print(prices.shape)\n",
    "print('-'*25)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging prices \n",
    "with timer():\n",
    "    train = train.merge(prices, on=['item_id','store_id','wm_yr_wk'], how='inner')\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del prices; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']:\n",
    "    train[col] = train[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT FORGET TO ADD SNAP DATA\n",
    "with timer():\n",
    "    train = train.merge(snap, on=['d', 'state_id'], how='left')\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del snap; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 0  >>>  id based nonzero statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With this id is obsolete\n",
    "f0 = train[train['sales'] > 0 ].groupby('id').agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "f0.columns = [ f[0] if f[1] == '' else 'id_nonzero_' + f[0] + '_' + f[1] for f in f0.columns ]\n",
    "\n",
    "print(f0.shape)\n",
    "print('-'*25)\n",
    "f0.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging f0\n",
    "with timer():\n",
    "    train = train.merge(f0, on='id', how='left')\n",
    "\n",
    "del f0; gc.collect()\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1  >>>  id based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With this id is obsolete\n",
    "f1 = train.groupby('id').agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "f1.columns = [ f[0] if f[1] == '' else 'id_' + f[0] + '_' + f[1] for f in f1.columns ]\n",
    "\n",
    "print(f1.shape)\n",
    "print('-'*25)\n",
    "f1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging f1\n",
    "with timer():\n",
    "    train = train.merge(f1, on='id', how='left')\n",
    "\n",
    "del f1; gc.collect()\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2  >>>  item_id based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With this item_id is obsolete\n",
    "f2 = train.groupby('item_id').agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "f2.columns = [ f[0] if f[1] == '' else 'item_id_' + f[0] + '_' + f[1] for f in f2.columns ]\n",
    "\n",
    "print(f2.shape)\n",
    "print('-'*25)\n",
    "f2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging f2\n",
    "with timer():\n",
    "    train = train.merge(f2, on='item_id', how='left')\n",
    "\n",
    "del f2; gc.collect()\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3  >>>  dept_id based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With this dept_id is obsolete\n",
    "f3 = train.groupby('dept_id').agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "f3.columns = [ f[0] if f[1] == '' else 'dept_id_' + f[0] + '_' + f[1] for f in f3.columns ]\n",
    "\n",
    "print(f3.shape)\n",
    "print('-'*25)\n",
    "f3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging f3\n",
    "with timer():\n",
    "    train = train.merge(f3, on='dept_id', how='left')\n",
    "\n",
    "del f3; gc.collect()\n",
    "\n",
    "print(train.shape)\n",
    "print('-'*25)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "\n",
    "for store in store_list:\n",
    "    with timer():\n",
    "        print(store)\n",
    "        train[train['store_id'] == store].to_csv(f'{path}FE3/Train_Data_2_{store}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORE BASED FEATURE ENGINEERING AND MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "print('** M5 - FORECASTING - ACCURACY **')\n",
    "print(\"-\"*25)\n",
    "\n",
    "import gc\n",
    "import os\n",
    "for x in os.listdir(\"input/\"):\n",
    "    print(x)\n",
    "print(\"-\"*25)\n",
    "\n",
    "path = 'input/'\n",
    "out_path = 'output/FE3/'\n",
    "\n",
    "print (\"Initiation is DONE !!\")\n",
    "print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SOME USEFUL FUNCTIONS\n",
    "\n",
    "## TIMER\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer():\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print( 'Runtime: ' + str(round(t1-t0,2)) + ' sn' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Feature 4 >>> id-monthdaily based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature4(train):\n",
    "    f4 = train.groupby(['id', 'day']).agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "    f4.columns = [ f[0] if f[1] == '' else 'id_day_' +  f[0] + '_' + f[1] for f in f4.columns ]\n",
    "\n",
    "    with timer():\n",
    "        train = train.merge(f4, on=['id', 'day'], how='left')\n",
    "\n",
    "    del f4; gc.collect()\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Feature 5 >>> id-weekdaily based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature5(train):\n",
    "    f5 = train.groupby(['id', 'wday']).agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "    f5.columns = [ f[0] if f[1] == '' else 'id_wday_' +  f[0] + '_' + f[1] for f in f5.columns ]\n",
    "    \n",
    "    with timer():\n",
    "        train = train.merge(f5, on=['id', 'wday'], how='left')\n",
    "\n",
    "    del f5; gc.collect()\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Feature 6 >>> id-monthly based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature6(train):\n",
    "    f6 = train.groupby(['id', 'month']).agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "    f6.columns = [ f[0] if f[1] == '' else 'id_month_' + f[0] + '_' + f[1] for f in f6.columns ]\n",
    "\n",
    "    with timer():\n",
    "        train = train.merge(f6, on=['id', 'month'], how='left')\n",
    "\n",
    "    del f6; gc.collect()\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Feature 7 >>> event based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature7(train):\n",
    "    \n",
    "    event_cat = ['Cultural', 'National', 'Religious', 'Sporting']\n",
    "\n",
    "    for event in event_cat:\n",
    "\n",
    "        with timer():\n",
    "            f7 = train[train[f'{event}'] > 0 ].groupby(['id']).agg({ 'sales': ['mean', 'std'] }).reset_index()\n",
    "            f7.columns = [ f[0] if f[1] == '' else f'id_{event}_' + f[0] + '_' + f[1] for f in f7.columns ]\n",
    "            f7[f'{event}'] = 1\n",
    "\n",
    "            train = train.merge(f7, on=['id', f'{event}'], how='left')\n",
    "            new_cols = [col for col in train.columns if col.startswith(f'id_{event}_')]\n",
    "\n",
    "            for c in new_cols:\n",
    "                train[c] = train[c].fillna(0)\n",
    "\n",
    "            del f7; gc.collect()\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model, store, pred, y_true, param_name, submission, feature_columns_to_keep):\n",
    "\n",
    "    print('-'*25)\n",
    "    print( 'MODEL PREDICTION BEGAN ...' )\n",
    "    print('-'*25)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(pred[feature_columns_to_keep])\n",
    "    x_pred = pred.copy()\n",
    "\n",
    "    x_pred['y_true'] = y_true\n",
    "    x_pred['y_pred'] = y_pred\n",
    "    x_pred['y_pred'] = x_pred['y_pred'].apply(lambda x: 0 if x<0 else x)\n",
    "    \n",
    "    \n",
    "    sns.lineplot(x='d', y='y_true', data=x_pred[ ( x_pred['id'] == f'FOODS_3_013_{store}' ) ] , color='r')\n",
    "    sns.lineplot(x='d', y='y_pred', data=x_pred[ ( x_pred['id'] == f'FOODS_3_013_{store}' ) ] , color='b')\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(x='d', y='y_true', data=x_pred[ ( x_pred['id'] == f'HOUSEHOLD_2_102_{store}' ) ], color='r')\n",
    "    sns.lineplot(x='d', y='y_pred', data=x_pred[ ( x_pred['id'] == f'HOUSEHOLD_2_102_{store}' ) ], color='b')\n",
    "    plt.show()    \n",
    "\n",
    "    \n",
    "    res_val  = x_pred[x_pred['d'] <= 1941][['id', 'd', 'y_pred']].copy()\n",
    "    res_eval = x_pred[x_pred['d'] > 1941][['id', 'd', 'y_pred']].copy() \n",
    "\n",
    "    res_val['id']  = res_val['id'].astype(str)  + '_validation'\n",
    "    res_eval['id'] = res_eval['id'].astype(str)  + '_evaluation'\n",
    "\n",
    "    res_val             = res_val.pivot_table(index='id', columns='d', values='y_pred').reset_index()\n",
    "    res_eval            = res_eval.pivot_table(index='id', columns='d', values='y_pred').reset_index()\n",
    "    res_eval.columns    = res_val.columns\n",
    "\n",
    "    res            = pd.concat([res_val, res_eval], axis=0, sort=False, ignore_index=True)\n",
    "    res            = submission[['id']].merge(res, on='id', how='inner')\n",
    "    res.columns    = submission.columns\n",
    "\n",
    "    res.to_csv(f'{out_path}model_lgbm_2_{store}_' + param_name + '.csv', index=False)\n",
    "\n",
    "    \n",
    "    del x_pred, y_pred, res_val, res_eval, res; gc.collect()\n",
    "    \n",
    "    print('-'*25)\n",
    "    print( 'MODEL PREDICTION COMPLETED !!!' )\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real Deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv( f'{path}sample_submission.csv')\n",
    "\n",
    "store_list = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "\n",
    "for store in store_list:\n",
    "    train = pd.read_csv(f'{path}FE3/Train_Data_2_{store}.csv')\n",
    "    \n",
    "    train = feature4(train)\n",
    "    train = feature5(train)\n",
    "    train = feature6(train)\n",
    "    train = feature7(train)\n",
    "    \n",
    "    cols_to_drop = ['item_id', 'store_id', 'state_id', 'wm_yr_wk']\n",
    "    event_cat = ['Cultural', 'National', 'Religious', 'Sporting']\n",
    "    \n",
    "    train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    train.drop(event_cat, axis=1, inplace=True)\n",
    "    \n",
    "    cat_cols = ['id', 'dept_id', 'cat_id' ]\n",
    "    for col in cat_cols:\n",
    "        train[col] = train[col].astype('category') \n",
    "\n",
    "    pred = train[train['d'] > 1913]\n",
    "    y_true = pred['sales']\n",
    "    pred = pred.drop('sales', axis=1)\n",
    "\n",
    "    valid = train[ (train['d'] <= 1941) & (train['d'] > 1913) ]\n",
    "    y_valid = valid['sales']\n",
    "    x_valid = valid.drop('sales', axis=1)\n",
    "\n",
    "    x_train = train.drop('sales', axis=1)\n",
    "    y_train = train['sales']\n",
    "\n",
    "    feature_columns_to_keep = [col for col in x_train.columns if col != 'd' ] # if ((col != 'id') & (col != 'd')) ]\n",
    "\n",
    "    train_data = lgb.Dataset( x_train[feature_columns_to_keep], categorical_feature=cat_cols, label = y_train, free_raw_data=False )\n",
    "    valid_data = lgb.Dataset( x_valid[feature_columns_to_keep], categorical_feature=cat_cols, label = y_valid, free_raw_data=False )\n",
    "\n",
    "    del valid, x_train, x_valid, y_train, y_valid ; gc.collect()\n",
    "    \n",
    "    \n",
    "    res_rmse = pd.DataFrame()\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = feature_columns_to_keep\n",
    "\n",
    "\n",
    "    obj_list   = [ 'regression', 'poisson']   # , 'tweedie' \n",
    "    boost_list = [ 'gbdt']\n",
    "    lr_list    = [ 0.7, 0.5, 0.3 ]\n",
    "    l1_list    = [ 0 ]\n",
    "    l2_list    = [ 0 ]\n",
    "\n",
    "    i=0\n",
    "\n",
    "\n",
    "    for obj in obj_list:\n",
    "        for boost in boost_list:\n",
    "            for lr in lr_list: \n",
    "                for l1 in l1_list:\n",
    "                    for l2 in l2_list:\n",
    "                        i += 1\n",
    "\n",
    "                        params = {\n",
    "                        # 'regression', 'regression_l1', 'tweedie',  'poisson', 'quantile', 'gamma', 'multiclass', 'cross_entropy'\n",
    "                        'objective'         : obj        ,\n",
    "\n",
    "                        #\n",
    "                        # 'num_class'         : len(set(train_data.label)) ,\n",
    "\n",
    "                        # 'gbdt', 'rf', 'dart', 'goss'\n",
    "                        'boosting'          : boost      ,\n",
    "\n",
    "                        # number of boosting iterations (100)  - [0, )\n",
    "                        'num_iterations'    : 6000        ,    \n",
    "\n",
    "                        # shrinkage rate (0.1) - [0, )\n",
    "                        'learning_rate'     : lr         ,\n",
    "\n",
    "                        # 'serial', 'feature', 'data', 'voting'\n",
    "                        'tree_learner'      : 'serial'   ,\n",
    "\n",
    "                        # if there are too many rows make it True, if there are too many columns make 'force_col_wise' option True\n",
    "                        # Never make them both True\n",
    "                        'force_row_wise'    : True       ,\n",
    "\n",
    "                        # if the data is small use it otherwise don't touch it\n",
    "                        'max_depth'         : -1         ,\n",
    "\n",
    "                        # 20 - [0, )\n",
    "                        'min_data_in_leaf'  : 31         , \n",
    "\n",
    "                        # randomly select part of data without resampling, to enable bagging it must be smaller than 1 (0,1]\n",
    "                        # 'bagging_fraction'  : 1          ,\n",
    "\n",
    "                        # to enable bagging bagging_freq must be nonzero [0, )\n",
    "                        #'bagging_freq'      : 0          ,\n",
    "\n",
    "                        # randomly select features, to enable featuring it must be smaller than 1 (0,1]\n",
    "                        # 'feature_fraction'  : 1          ,\n",
    "\n",
    "                        # stops training if one metric of one validation data does not improve in the last x rounds (0)\n",
    "                        'early_stopping_round' : 1000     ,\n",
    "\n",
    "                        # (0)\n",
    "                        'lambda_l1'         : l1         ,\n",
    "\n",
    "                        # (0)\n",
    "                        'lambda_l2'         : l2          ,\n",
    "\n",
    "                        #controls the level of LGBM verbosity\n",
    "                        'verbosity'         : 1          ,  \n",
    "\n",
    "                        # 'rmse', 'auc', 'l1', 'l2', 'tweedie', 'poissson', 'multi_logloss'..\n",
    "                        'metric'            : 'rmse'      ,\n",
    "\n",
    "                        # Use these 3 parameters to handle memory error\n",
    "                        # -----------------------------------------------------\n",
    "                        # max number of leaves in one tree (31) - [1,131072]\n",
    "                        'num_leaves'              : 128    , \n",
    "                        # max cache size in MB for historical histogram (-1)\n",
    "                        #'histogram_pool_size'     : 128   ,\n",
    "                        # max number of bins that feature values will be bucketed in (255) - \n",
    "                        #'max_bin'                 : 31    ,\n",
    "                        # -----------------------------------------------------\n",
    "\n",
    "                        'random_state': 28,\n",
    "\n",
    "                        }\n",
    "\n",
    "                        with timer():\n",
    "                            model = lgb.train(params, train_data, 1000, valid_sets = [valid_data], categorical_feature=cat_cols, verbose_eval=100 ) \n",
    "                            mtrc_rmse = model.best_score['valid_0']['rmse']\n",
    "                            param_name = f'{i}_{obj}_{boost}_{str(lr)}_{str(l1)}_{str(l2)}_{str(mtrc_rmse)}'\n",
    "\n",
    "\n",
    "                        model.save_model(f'{out_path}model_lgbm_2_{store}_' + param_name + '.txt')\n",
    "\n",
    "                        res_rmse = res_rmse.append( pd.DataFrame( {  'i': [i],\n",
    "                                                 'objective': [obj],\n",
    "                                                 'boosting': [boost],\n",
    "                                                 'learning_rate':[lr], \n",
    "                                                 'l1':[l1], \n",
    "                                                 'l2':[l2], \n",
    "                                                 'RMSE': [mtrc_rmse]  } ) )\n",
    "\n",
    "                        feature_importances[param_name] = model.feature_importance()               \n",
    "\n",
    "                        plt.figure( figsize=(12,8) )\n",
    "                        sns.barplot( x    = param_name,\n",
    "                                     y    = 'feature',\n",
    "                                     data = feature_importances.sort_values(param_name, ascending = False ) )\n",
    "                        plt.title('LGBM Feature Importance ' + store + ' ' + param_name)\n",
    "                        plt.show()\n",
    "\n",
    "                        predictor(model, store, pred, y_true, param_name, submission, feature_columns_to_keep)\n",
    "\n",
    "                        print('-'*25)\n",
    "                        print( res_rmse )\n",
    "                        print('-'*25)\n",
    "\n",
    "\n",
    "    res_rmse.to_excel(f'{out_path}res_rmse_{store}.xlsx', index=False)                 \n",
    "    feature_importances.to_excel(f'{out_path}feature_importances_{store}.xlsx', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in os.listdir(\"output/\"):\n",
    "    if x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = [e for e in os.listdir(\"output/\") if ((\"model_lgbm_2\" in e ) & (\"regression_gbdt_0.5\" in e ) & (\".csv\" in e ) ) ]\n",
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for doc in matching:\n",
    "    df = df.append( pd.read_csv('output/' + doc) )\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['F1'].notnull() ]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [col for col in df.columns if col.startswith(\"F\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in col_list:\n",
    "    df[col] = df[col].apply(lambda x: 0 if x<0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_lgbm_2_1_regression_gbdt_0.5_0_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': 1,\n",
    "                } \n",
    "\n",
    "# Let's look closer on params\n",
    "\n",
    "## 'boosting_type': 'gbdt'\n",
    "# we have 'goss' option for faster training\n",
    "# but it normally leads to underfit.\n",
    "# Also there is good 'dart' mode\n",
    "# but it takes forever to train\n",
    "# and model performance depends \n",
    "# a lot on random factor \n",
    "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
    "\n",
    "## 'objective': 'tweedie'\n",
    "# Tweedie Gradient Boosting for Extremely\n",
    "# Unbalanced Zero-inflated Data\n",
    "# https://arxiv.org/pdf/1811.10192.pdf\n",
    "# and many more articles about tweediie\n",
    "#\n",
    "# Strange (for me) but Tweedie is close in results\n",
    "# to my own ugly loss.\n",
    "# My advice here - make OWN LOSS function\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
    "# I think many of you already using it (after poisson kernel appeared) \n",
    "# (kagglers are very good with \"params\" testing and tuning).\n",
    "# Try to figure out why Tweedie works.\n",
    "# probably it will show you new features options\n",
    "# or data transformation (Target transformation?).\n",
    "\n",
    "## 'tweedie_variance_power': 1.1\n",
    "# default = 1.5\n",
    "# set this closer to 2 to shift towards a Gamma distribution\n",
    "# set this closer to 1 to shift towards a Poisson distribution\n",
    "# my CV shows 1.1 is optimal \n",
    "# but you can make your own choice\n",
    "\n",
    "## 'metric': 'rmse'\n",
    "# Doesn't mean anything to us\n",
    "# as competition metric is different\n",
    "# and we don't use early stoppings here.\n",
    "# So rmse serves just for general \n",
    "# model performance overview.\n",
    "# Also we use \"fake\" validation set\n",
    "# (as it makes part of the training set)\n",
    "# so even general rmse score doesn't mean anything))\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
    "\n",
    "## 'subsample': 0.5\n",
    "# Serves to fight with overfit\n",
    "# this will randomly select part of data without resampling\n",
    "# Chosen by CV (my CV can be wrong!)\n",
    "# Next kernel will be about CV\n",
    "\n",
    "##'subsample_freq': 1\n",
    "# frequency for bagging\n",
    "# default value - seems ok\n",
    "\n",
    "## 'learning_rate': 0.03\n",
    "# Chosen by CV\n",
    "# Smaller - longer training\n",
    "# but there is an option to stop \n",
    "# in \"local minimum\"\n",
    "# Bigger - faster training\n",
    "# but there is a chance to\n",
    "# not find \"global minimum\" minimum\n",
    "\n",
    "## 'num_leaves': 2**11-1\n",
    "## 'min_data_in_leaf': 2**12-1\n",
    "# Force model to use more features\n",
    "# We need it to reduce \"recursive\"\n",
    "# error impact.\n",
    "# Also it leads to overfit\n",
    "# that's why we use small \n",
    "# 'max_bin': 100\n",
    "\n",
    "## l1, l2 regularizations\n",
    "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "# Good tiny explanation\n",
    "# l2 can work with bigger num_leaves\n",
    "# but my CV doesn't show boost\n",
    "                    \n",
    "## 'n_estimators': 1400\n",
    "# CV shows that there should be\n",
    "# different values for each state/store.\n",
    "# Current value was chosen \n",
    "# for general purpose.\n",
    "# As we don't use any early stopings\n",
    "# careful to not overfit Public LB.\n",
    "\n",
    "##'feature_fraction': 0.5\n",
    "# LightGBM will randomly select \n",
    "# part of features on each iteration (tree).\n",
    "# We have maaaany features\n",
    "# and many of them are \"duplicates\"\n",
    "# and many just \"noise\"\n",
    "# good values here - 0.5-0.7 (by CV)\n",
    "\n",
    "## 'boost_from_average': False\n",
    "# There is some \"problem\"\n",
    "# to code boost_from_average for \n",
    "# custom loss\n",
    "# 'True' makes training faster\n",
    "# BUT carefull use it\n",
    "# https://github.com/microsoft/LightGBM/issues/1514\n",
    "# not our case but good to know cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
